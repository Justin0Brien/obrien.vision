---
layout: post
title: "Computer says yes: why Iâ€™d prefer to be fired by AI I"
date: 2025-09-29
categories: [linkedin]
tags: [linkedin, social-media]
author: "Justin O'Brien"
excerpt: "Computer says yes: why Iâ€™d prefer to be fired by AI I was made redundant by a panel so mysterious it made the REF panel look like Eurovision. No minutes. No rationale. When I asked for the reasoning, the collective re..."
linkedin_url: "https://www.linkedin.com/feed/update/urn:li:activity:7378352903264120832"
image: "/assets/images/posts/2025-09-29-computer-says-yes-why-i-d-prefer-to-be-fired-by-ai-i/1757503112290.jpg"
thumbnail: "/assets/images/posts/2025-09-29-computer-says-yes-why-i-d-prefer-to-be-fired-by-ai-i/1757503112290.jpg"
---

![LinkedIn post image](/assets/images/posts/2025-09-29-computer-says-yes-why-i-d-prefer-to-be-fired-by-ai-i/1757503112290.jpg)

*Originally posted on LinkedIn on September 29, 2025.*

Computer says yes: why Iâ€™d prefer to be fired by AI I was made redundant by a panel so mysterious it made theÂ REF panel look like Eurovision. No minutes. No rationale. When I asked for the reasoning, the collective response was administrative amnesia: everyone present could recall nothing and apparently wrote down even less. I left with a letter and a shrug. Hereâ€™s the heresy:Â I would rather have been fired by AI. Not because I want a robot to decide my livelihood, but because an AI-assisted process (done properly) would have forced theÂ humansÂ to be accountable. Computers donâ€™t get tired, forget to take notes, or lose the rubric halfway through. Used as a clerk with a perfect memory, AI could have given me what I didnâ€™t get: a neutral, detailed, auditable explanation. What that looks like in the real world: ğŸ“Â Criteria frozen up front.Â Role requirements, weightings, and acceptable evidence publishedÂ beforeÂ anyone reads a self-assessment. If the ruler changes after youâ€™ve seen the answers, it isnâ€™t a ruler. ğŸ§¾Â Evidence in, waffle out.Â The system compiles the relevant data and maps it to the criteria. No vibes, no prose gloss. ğŸªŸÂ Transparent summaries, human decisions.Â AI produces side-by-side summaries with links to source documents; a named human panel makes the call â€” on record. âš–ï¸Â Bias and adverse-impact checks.Â Automatically flagged and reviewedÂ beforeÂ anything is final. ğŸ—£ï¸Â Reasons you can read.Â Point-by-point explanations tied to evidence, written in plain English and sent to the person affected. ğŸ—ƒï¸Â An audit trail that exists.Â Model/version, prompts used, who reviewed what, when. Boring? Yes. Also the difference between â€œwe can show our workingsâ€ and â€œwe, er, canâ€™t find them.â€ âœï¸Â Human signature that means something.Â A single accountable sign-off that says, in effect, â€œIâ€™ve read this, I stand by it.â€ Notice what AI isÂ notÂ doing here: it isnâ€™t choosing who goes. Itâ€™s the tireless administrator, sorting, cross-referencing, and ensuring that when a difficult judgement is made, it can be explained without theatrics or telepathy. In my case, the process produced no reasons and no record. If your system canâ€™t explain itself, itâ€™s not a system; itâ€™s a shrug with stationery. So yes:Â computer says yesÂ - to criteria, reasons, and a paper trail. Then a human says the hard bit out loud and owns it. If youâ€™re designing or defending these processes: could you explain every step to the person across the table and feel comfortable doing it again in a year? If not, you donâ€™t need fewer tools; you need more daylight. #HigherEducation #UKHE #Ethics #AIinHE #Accountability #Transparency #GoodGovernance #HR #Leadership

[View on LinkedIn](https://www.linkedin.com/feed/update/urn:li:activity:7378352903264120832)
